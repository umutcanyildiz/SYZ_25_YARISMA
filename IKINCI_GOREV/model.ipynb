{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e547b9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import random\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "# Cosine Annealing iÃ§in artÄ±k ReduceLROnPlateau'ya gerek yok\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR \n",
    "import timm\n",
    "\n",
    "# ==================== AYARLAR ====================\n",
    "MODEL_ADI = 'tf_efficientnetv2_m.in21k'\n",
    "MODEL_PATH = f\"{MODEL_ADI}_cosine_scheduler_adcverili.pth\" # Yeni model adÄ±\n",
    "\n",
    "# Hiperparametreler\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 100 # Cosine scheduler ile biraz daha uzun eÄŸitmek genellikle faydalÄ±dÄ±r\n",
    "PATIENCE = 10 # SabrÄ± da artÄ±rabiliriz\n",
    "LR_BACKBONE = 2e-5 # Biraz daha agresif bir baÅŸlangÄ±Ã§\n",
    "LR_HEAD = 2e-3\n",
    "WEIGHT_DECAY = 1e-3\n",
    "\n",
    "# Veri YollarÄ± ve DiÄŸer Ayarlar (AynÄ±)\n",
    "BASE_DATA_DIR = \"/home/comp5/ARTEK/SYZ_25/SYZ_25_Egitim/IKINCI_GOREV/yeni_veri_imbalancli_splitted\"\n",
    "TRAIN_DATA_ROOT = os.path.join(BASE_DATA_DIR, \"train\")\n",
    "VAL_DATA_ROOT = os.path.join(BASE_DATA_DIR, \"val\")\n",
    "CLASS_NAMES = [\"HiperakutAkut\", \"NormalKronik\", \"Subakut\"]\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "RANDOM_SEED = 42\n",
    "random.seed(RANDOM_SEED); np.random.seed(RANDOM_SEED); torch.manual_seed(RANDOM_SEED)\n",
    "# =================================================\n",
    "\n",
    "# ==== Dataset, build_sequences, create_model (AynÄ±) ====\n",
    "class MRDataset(Dataset):\n",
    "    def __init__(self, sequences, transform): self.sequences = sequences; self.transform = transform\n",
    "    def __len__(self): return len(self.sequences)\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.sequences[idx]\n",
    "        image = self.transform(Image.open(img_path[0]).convert(\"RGB\"))\n",
    "        return image, torch.tensor(label)\n",
    "def build_sequences(data_root):\n",
    "    sequences = []; class_to_idx = {cls: i for i, cls in enumerate(CLASS_NAMES)}\n",
    "    for cls in CLASS_NAMES:\n",
    "        img_paths = sorted(glob.glob(os.path.join(data_root, cls, \"*.png\")))\n",
    "        for path in img_paths: sequences.append(([path], class_to_idx[cls]))\n",
    "    return sequences\n",
    "def create_model(model_name, num_classes):\n",
    "    model = timm.create_model(model_name, pretrained=True, num_classes=num_classes)\n",
    "    return model.to(DEVICE)\n",
    "\n",
    "# ==== EÄŸitim Fonksiyonu (Cosine Scheduler ile GÃ¼ncellendi) ====\n",
    "def train():\n",
    "    temp_model = create_model(MODEL_ADI, len(CLASS_NAMES)); data_config = timm.data.resolve_data_config({}, model=temp_model)\n",
    "    img_size = data_config['input_size'][1]; del temp_model\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((img_size, img_size)), transforms.TrivialAugmentWide(), transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=data_config['mean'], std=data_config['std'])\n",
    "    ])\n",
    "    train_seq = build_sequences(TRAIN_DATA_ROOT); val_seq = build_sequences(VAL_DATA_ROOT)\n",
    "    labels = [label for _, label in train_seq]; class_counts = np.bincount(labels, minlength=len(CLASS_NAMES))\n",
    "    class_weights_tensor = torch.tensor([sum(class_counts) / c if c > 0 else 0 for c in class_counts], dtype=torch.float).to(DEVICE)\n",
    "    train_dataset = MRDataset(train_seq, transform); val_dataset = MRDataset(val_seq, transform)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, num_workers=4, pin_memory=True)\n",
    "    \n",
    "    model = create_model(MODEL_ADI, len(CLASS_NAMES))\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights_tensor, label_smoothing=0.1)\n",
    "\n",
    "    # Discriminative LR (AynÄ±)\n",
    "    head_params = list(model.classifier.parameters())\n",
    "    backbone_params = [p for p in model.parameters() if not any(p is hp for hp in head_params)]\n",
    "    param_groups = [{'params': backbone_params, 'lr': LR_BACKBONE}, {'params': head_params, 'lr': LR_HEAD}]\n",
    "    optimizer = torch.optim.AdamW(param_groups, weight_decay=WEIGHT_DECAY)\n",
    "    \n",
    "    # YENÄ° SCHEDULER\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=len(train_loader) * EPOCHS, eta_min=1e-7)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    \n",
    "    print(f\"\\nðŸš€ {MODEL_ADI} modeli ile Cosine Annealing Scheduler ile eÄŸitim baÅŸlÄ±yor...\\n\")\n",
    "\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        model.train(); train_loss, train_acc = 0, 0\n",
    "        \n",
    "        # tqdm'Ä± dÃ¶ngÃ¼nÃ¼n dÄ±ÅŸÄ±na alarak daha temiz bir ilerleme Ã§ubuÄŸu\n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch}/{EPOCHS}\")\n",
    "        for x, y in pbar:\n",
    "            x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(x)\n",
    "            loss = criterion(out, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # Her adÄ±mdan sonra scheduler.step() Ã§aÄŸÄ±rÄ±yoruz\n",
    "            scheduler.step()\n",
    "            \n",
    "            train_acc += (out.argmax(1) == y).sum().item()\n",
    "            train_loss += loss.item() * x.size(0)\n",
    "            pbar.set_postfix(lr=optimizer.param_groups[0]['lr'], loss=loss.item())\n",
    "\n",
    "        train_acc /= len(train_dataset); train_loss /= len(train_dataset)\n",
    "        \n",
    "        # DoÄŸrulama dÃ¶ngÃ¼sÃ¼...\n",
    "        model.eval(); val_loss, val_acc = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for x, y in val_loader:\n",
    "                x, y = x.to(DEVICE), y.to(DEVICE); out = model(x); loss = criterion(out, y)\n",
    "                val_acc += (out.argmax(1) == y).sum().item(); val_loss += loss.item() * x.size(0)\n",
    "        val_acc /= len(val_dataset); val_loss /= len(val_dataset)\n",
    "        \n",
    "        print(f\"ðŸ“Š Epoch {epoch:02d} | Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss; torch.save(model.state_dict(), MODEL_PATH); print(f\"   âœ¨ Model kaydedildi. En iyi Val Loss: {best_val_loss:.4f}\"); patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= PATIENCE: print(f\"â›” Early stopping at epoch {epoch}.\"); break\n",
    "            \n",
    "    print(f\"âœ… EÄŸitim tamamlandÄ±. En iyi model ({MODEL_ADI}): {MODEL_PATH}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c126e757",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import random\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import timm\n",
    "from sklearn.metrics import confusion_matrix, f1_score, accuracy_score, classification_report\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "MODEL_ADI = '/home/comp5/ARTEK/SYZ_25/SYZ_25_Egitim/IKINCI_GOREV/veri_png_splitle_egitilen/tf_efficientnetv2_m.in21k'\n",
    "MODEL_PATH = f\"{MODEL_ADI}_cosine_scheduler_best_olacak.pth\" # Yeni model adÄ±\n",
    "\n",
    "# Hiperparametreler\n",
    "BATCH_SIZE = 4 # Test iÃ§in BATCH_SIZE'Ä± artÄ±rabilirsiniz, bu hÄ±zÄ± etkiler\n",
    "\n",
    "# Veri YollarÄ±\n",
    "BASE_DATA_DIR = \"/home/comp5/ARTEK/SYZ_25/SYZ_25_Egitim/IKINCI_GOREV/VERI/veri_png_split\"\n",
    "TEST_DATA_ROOT = os.path.join(BASE_DATA_DIR, \"test\")\n",
    "\n",
    "# DiÄŸer Ayarlar\n",
    "CLASS_NAMES = [\"HiperakutAkut\", \"NormalKronik\", \"Subakut\"]\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "RANDOM_SEED = 42\n",
    "random.seed(RANDOM_SEED); np.random.seed(RANDOM_SEED); torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "class MRDataset(Dataset):\n",
    "    def __init__(self, sequences, transform):\n",
    "        self.sequences = sequences\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.sequences[idx]\n",
    "        image = self.transform(Image.open(img_path[0]).convert(\"RGB\"))\n",
    "        return image, torch.tensor(label)\n",
    "\n",
    "def build_sequences(data_root):\n",
    "    sequences = []\n",
    "    class_to_idx = {cls: i for i, cls in enumerate(CLASS_NAMES)}\n",
    "    for cls in CLASS_NAMES:\n",
    "        img_paths = sorted(glob.glob(os.path.join(data_root, cls, \"*.png\")))\n",
    "        for path in img_paths:\n",
    "            sequences.append(([path], class_to_idx[cls]))\n",
    "    return sequences\n",
    "\n",
    "def create_model(model_name, num_classes):\n",
    "    model = timm.create_model(model_name, pretrained=False, num_classes=num_classes)\n",
    "    return model.to(DEVICE)\n",
    "\n",
    "def test():\n",
    "    temp_model = create_model(MODEL_ADI, len(CLASS_NAMES))\n",
    "    data_config = timm.data.resolve_data_config({}, model=temp_model)\n",
    "    img_size = data_config['input_size'][1]\n",
    "    del temp_model\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((img_size, img_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=data_config['mean'], std=data_config['std'])\n",
    "    ])\n",
    "\n",
    "    print(f\"ðŸ“‚ Test verisi yÃ¼kleniyor: {TEST_DATA_ROOT}\")\n",
    "    test_seq = build_sequences(TEST_DATA_ROOT)\n",
    "    test_dataset = MRDataset(test_seq, transform)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)\n",
    "    \n",
    "    print(f\"ðŸ§ª Toplam {len(test_dataset)} test Ã¶rneÄŸi bulundu.\")\n",
    "\n",
    "    model = create_model(MODEL_ADI, len(CLASS_NAMES))\n",
    "    print(f\"ðŸ’¾ Model aÄŸÄ±rlÄ±klarÄ± yÃ¼kleniyor: {MODEL_PATH}\")\n",
    "    model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
    "    model.eval() # Modeli DEÄžERLENDÄ°RME MODUNA AL (Ã§ok Ã¶nemli!)\n",
    "\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "\n",
    "    print(\"ðŸš€ DeÄŸerlendirme baÅŸlÄ±yor...\")\n",
    "    with torch.no_grad(): # Gradient hesaplamayÄ± kapat\n",
    "        for images, labels in tqdm(test_loader, desc=\"Test Ediliyor\"):\n",
    "            images = images.to(DEVICE)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            predictions = torch.argmax(outputs, 1)\n",
    "\n",
    "            all_labels.extend(labels.numpy())\n",
    "            all_preds.extend(predictions.cpu().numpy())\n",
    "\n",
    "    # Metrikleri Hesapla\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    macro_f1 = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    report = classification_report(all_labels, all_preds, target_names=CLASS_NAMES, zero_division=0)\n",
    "\n",
    "    # SonuÃ§larÄ± YazdÄ±r\n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(\"ðŸ“Š DETAYLI TEST SONUÃ‡LARI\")\n",
    "    print(\"=\"*40)\n",
    "    print(f\"DoÄŸruluk (Accuracy): {accuracy:.4f}\")\n",
    "    print(f\"Makro F1 Skoru:      {macro_f1:.4f}\")\n",
    "    print(\"\\nSÄ±nÄ±flandÄ±rma Raporu (Classification Report):\\n\")\n",
    "    print(report)\n",
    "    print(\"=\"*40)\n",
    "\n",
    "    # KarÄ±ÅŸÄ±klÄ±k Matrisini (Confusion Matrix) GÃ¶rselleÅŸtir\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=CLASS_NAMES, yticklabels=CLASS_NAMES)\n",
    "    plt.xlabel('Tahmin Edilen SÄ±nÄ±f (Predicted Class)', fontsize=12)\n",
    "    plt.ylabel('GerÃ§ek SÄ±nÄ±f (True Class)', fontsize=12)\n",
    "    plt.title('KarÄ±ÅŸÄ±klÄ±k Matrisi (Confusion Matrix)', fontsize=14)\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python-3.10.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
