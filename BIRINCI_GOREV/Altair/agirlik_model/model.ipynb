{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee1f5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models, transforms, datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from tqdm import tqdm \n",
    "import os\n",
    "\n",
    "# ==== Cihaz AyarÄ± ====\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ==== Sabitler ====\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 20\n",
    "PATIENCE = 4\n",
    "MODEL_PATH = \"gul_resnet34.pth\"\n",
    "\n",
    "# ==== Veri DÃ¶nÃ¼ÅŸtÃ¼rÃ¼cÃ¼ ====\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "train_dataset = datasets.ImageFolder(\"/home/comp5/ARTEK/SYZ_25/ILK_GOREV/VERI/altair/train\", transform=transform)\n",
    "val_dataset   = datasets.ImageFolder(\"/home/comp5/ARTEK/SYZ_25/ILK_GOREV/VERI/altair/val\", transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "model = models.resnet34(pretrained=True)\n",
    "# model = models.resnet18(pretrained=True)\n",
    "# model = models.resnet50(pretrained=True)\n",
    "num_features = model.fc.in_features\n",
    "model.fc = nn.Linear(num_features, 1)\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "best_val_loss = np.inf\n",
    "early_stop_counter = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    train_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} [Train]\", leave=False)\n",
    "    for images, labels in train_bar:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device).float().unsqueeze(1)\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item() * images.size(0)\n",
    "        preds = (torch.sigmoid(outputs) > 0.5).float()\n",
    "        correct += (preds == labels).sum().item()\n",
    "\n",
    "        train_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    train_acc = correct / len(train_dataset)\n",
    "    train_loss /= len(train_dataset)\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    val_bar = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} [Val]\", leave=False)\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_bar:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device).float().unsqueeze(1)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item() * images.size(0)\n",
    "\n",
    "            preds = (torch.sigmoid(outputs) > 0.5).float()\n",
    "            correct += (preds == labels).sum().item()\n",
    "\n",
    "            val_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    val_acc = correct / len(val_dataset)\n",
    "    val_loss /= len(val_dataset)\n",
    "\n",
    "    print(f\"\\nðŸ“Š Epoch {epoch+1}/{EPOCHS} | \"\n",
    "          f\"Train Loss: {train_loss:.4f}, Acc: {train_acc:.4f} | \"\n",
    "          f\"Val Loss: {val_loss:.4f}, Acc: {val_acc:.4f}\")\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), MODEL_PATH)\n",
    "        early_stop_counter = 0\n",
    "        print(\"âœ“ En iyi model kaydedildi.\")\n",
    "    else:\n",
    "        early_stop_counter += 1\n",
    "        if early_stop_counter >= PATIENCE:\n",
    "            print(f\"ðŸ›‘ Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "print(\"ðŸŽ¯ EÄŸitim tamamlandÄ±. En iyi model kaydedildi:\", MODEL_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63f7380",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from torchvision import models, transforms, datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, accuracy_score, precision_score,\n",
    "    recall_score, f1_score, roc_auc_score, roc_curve, ConfusionMatrixDisplay\n",
    ")\n",
    "\n",
    "# ==== Ayarlar ====\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "MODEL_PATH = \"/home/comp5/ARTEK/SYZ_25/SYZ_25_Egitim/ILK_GOREV/nihai_modeller/best_resnet50_yeniveri_teknofest_model/best_resnet50_yeniveri_model.pth\"\n",
    "TEST_DIR = \"/home/comp5/ARTEK/SYZ_25/SYZ_25_Egitim/genel_veri/2025_Veri/test_veri_seti/ÃœNÄ°VERSÄ°TEVEÃœZERÄ°_EGÄ°TÄ°M 2_PNG/BT_TestSet/teknofest_test_hazir/test\"\n",
    "OUT_DIR = Path(\"denenm_dogrulama\")\n",
    "BATCH_SIZE = 32\n",
    "THRESHOLD = 0.5\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ==== Transform ====\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# ==== Dataset & Loader ====\n",
    "test_dataset = datasets.ImageFolder(TEST_DIR, transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "class_names = test_dataset.classes\n",
    "file_paths = [s[0] for s in test_dataset.samples]\n",
    "\n",
    "# ==== Model ====\n",
    "model = models.resnet50(pretrained=False)\n",
    "model.fc = nn.Linear(model.fc.in_features, 1)\n",
    "model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
    "model.to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "# ==== Tahminler ====\n",
    "all_probs, all_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(DEVICE)\n",
    "        labels = labels.to(DEVICE).float().unsqueeze(1)\n",
    "        outputs = model(images)\n",
    "        probs = torch.sigmoid(outputs)\n",
    "        all_probs.extend(probs.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "all_probs = np.array(all_probs).flatten()\n",
    "all_labels = np.array(all_labels).flatten()\n",
    "all_preds = (all_probs >= THRESHOLD).astype(float)\n",
    "\n",
    "# ==== Threshold Optimizasyonu ====\n",
    "def find_best_threshold(y_true, y_probs):\n",
    "    best_thresh = 0.5\n",
    "    best_score = 0\n",
    "    for thresh in np.linspace(0.1, 0.9, 100):\n",
    "        preds = (y_probs >= thresh).astype(int)\n",
    "        score = f1_score(y_true, preds, average='macro')\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_thresh = thresh\n",
    "    return best_thresh, best_score\n",
    "\n",
    "best_thresh, best_score = find_best_threshold(all_labels, all_probs)\n",
    "all_preds_best = (all_probs >= best_thresh).astype(int)\n",
    "\n",
    "# ==== Metrikler ====\n",
    "def compute_metrics(y_true, y_pred, y_probs, threshold):\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    prec = precision_score(y_true, y_pred)\n",
    "    rec = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    macro_f1 = f1_score(y_true, y_pred, average='macro')\n",
    "    roc_auc = roc_auc_score(y_true, y_probs)\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    class0_acc = cm[0, 0] / (cm[0, 0] + cm[0, 1])\n",
    "    class1_acc = cm[1, 1] / (cm[1, 0] + cm[1, 1])\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": float(acc),\n",
    "        \"precision\": float(prec),\n",
    "        \"recall\": float(rec),\n",
    "        \"f1_score\": float(f1),\n",
    "        \"macro_f1_score\": float(macro_f1),\n",
    "        \"roc_auc\": float(roc_auc),\n",
    "        \"class_0_acc\": float(class0_acc),\n",
    "        \"class_1_acc\": float(class1_acc),\n",
    "        \"threshold\": float(threshold),\n",
    "        \"num_samples\": int(len(y_true)),\n",
    "        \"class_labels\": class_names\n",
    "    }, cm\n",
    "\n",
    "metrics_default, cm_default = compute_metrics(all_labels, all_preds, all_probs, THRESHOLD)\n",
    "metrics_best, cm_best = compute_metrics(all_labels, all_preds_best, all_probs, best_thresh)\n",
    "\n",
    "# ==== JSON KayÄ±t ====\n",
    "with open(OUT_DIR / \"metrics.json\", \"w\") as f: json.dump(metrics_default, f, indent=2)\n",
    "with open(OUT_DIR / \"metrics_best.json\", \"w\") as f: json.dump(metrics_best, f, indent=2)\n",
    "\n",
    "# ==== Rapor CSV ====\n",
    "report = classification_report(all_labels, all_preds, target_names=class_names, output_dict=True, zero_division=0)\n",
    "pd.DataFrame(report).T.to_csv(OUT_DIR / \"class_report.csv\")\n",
    "\n",
    "report_best = classification_report(all_labels, all_preds_best, target_names=class_names, output_dict=True, zero_division=0)\n",
    "pd.DataFrame(report_best).T.to_csv(OUT_DIR / \"class_report_best.csv\")\n",
    "\n",
    "# ==== Tahmin CSV ====\n",
    "df = pd.DataFrame({\n",
    "    \"filepath\": file_paths,\n",
    "    \"y_true\": [class_names[int(x)] for x in all_labels],\n",
    "    \"y_pred_default\": [class_names[int(x)] for x in all_preds],\n",
    "    \"y_pred_best\": [class_names[int(x)] for x in all_preds_best],\n",
    "    \"prob_positive\": all_probs\n",
    "})\n",
    "df.to_csv(OUT_DIR / \"predictions.csv\", index=False)\n",
    "\n",
    "# ==== Confusion Matrix ====\n",
    "for name, cm, t in [(\"default\", cm_default, THRESHOLD), (\"best\", cm_best, best_thresh)]:\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
    "    disp.plot(cmap=plt.cm.Blues)\n",
    "    plt.title(f\"Confusion Matrix ({name.upper()} Threshold={t:.4f})\")\n",
    "    plt.savefig(OUT_DIR / f\"confusion_matrix_{name}.png\", dpi=150, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "# ==== ROC Curve ====\n",
    "fpr, tpr, _ = roc_curve(all_labels, all_probs)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label=f\"AUC = {metrics_best['roc_auc']:.3f}\")\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.savefig(OUT_DIR / \"roc_curve.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "\n",
    "# ==== GÃ¶rsel Grid ====\n",
    "print(\"ðŸ“· Grid gÃ¶rseller kaydediliyor...\")\n",
    "def save_image_grid(preds, name):\n",
    "    num_samples = len(file_paths)\n",
    "    grid_size = 100\n",
    "    num_chunks = (num_samples + grid_size - 1) // grid_size\n",
    "    img_size = (224, 224)\n",
    "\n",
    "    for chunk_idx in range(num_chunks):\n",
    "        start = chunk_idx * grid_size\n",
    "        end = min(start + grid_size, num_samples)\n",
    "        chunk_paths = file_paths[start:end]\n",
    "        chunk_true = all_labels[start:end]\n",
    "        chunk_pred = preds[start:end]\n",
    "\n",
    "        cols = int(round(math.sqrt(len(chunk_paths))))\n",
    "        rows = math.ceil(len(chunk_paths) / cols)\n",
    "        fig, axes = plt.subplots(rows, cols, figsize=(cols * 1.6, rows * 1.6))\n",
    "\n",
    "        if rows == 1:\n",
    "            axes = [axes]\n",
    "        axes = np.array(axes).reshape(-1)\n",
    "\n",
    "        for i, ax in enumerate(axes):\n",
    "            idx = start + i\n",
    "            if i < len(chunk_paths):\n",
    "                img_path = chunk_paths[i]\n",
    "                true_label = class_names[int(chunk_true[i])]\n",
    "                pred_label = class_names[int(chunk_pred[i])]\n",
    "                color = \"green\" if true_label == pred_label else \"red\"\n",
    "\n",
    "                try:\n",
    "                    img = Image.open(img_path).convert(\"RGB\").resize(img_size)\n",
    "                    ax.imshow(img)\n",
    "                except:\n",
    "                    ax.text(0.5, 0.5, \"IMAGE ERROR\", ha=\"center\", va=\"center\", fontsize=8)\n",
    "                    ax.set_facecolor(\"gray\")\n",
    "\n",
    "                ax.set_title(f\"T:{true_label}\\nP:{pred_label}\", fontsize=7, color=color)\n",
    "            ax.axis(\"off\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        out_file = OUT_DIR / f\"samples_{name}_{chunk_idx+1}.png\"\n",
    "        plt.savefig(out_file, dpi=150, bbox_inches=\"tight\")\n",
    "        plt.close()\n",
    "        print(f\"âœ“ {out_file.name} kaydedildi.\")\n",
    "\n",
    "# Kaydet\n",
    "save_image_grid(all_preds, \"default\")\n",
    "save_image_grid(all_preds_best, \"best\")\n",
    "\n",
    "print(f\"ðŸ TamamlandÄ±. Best threshold: {best_thresh:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python-3.10.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
