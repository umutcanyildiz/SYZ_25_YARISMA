{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24077f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_strokevit_r50_noarg.py\n",
    "# -*- coding: utf-8 -*-\n",
    "import os, math, random, json\n",
    "from dataclasses import dataclass\n",
    "from copy import deepcopy\n",
    "from typing import Tuple\n",
    "\n",
    "import numpy as np\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "from torchvision import datasets, transforms\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "from model_strokevit_r50 import StrokeViT_R50\n",
    "\n",
    "# =========================\n",
    "# Konfig (argparse YOK)\n",
    "# =========================\n",
    "@dataclass\n",
    "class CFG:\n",
    "    train_dir: str = \"/home/comp5/ARTEK/SYZ_25/ILK_GOREV/VERI/ilk_gorev_veri_split/train\"\n",
    "    val_dir:   str = \"/home/comp5/ARTEK/SYZ_25/ILK_GOREV/VERI/ilk_gorev_veri_split/val\"\n",
    "    save_path: str = \"outputs_strokevit_r50_best/strokevit_r50_best.pth\"\n",
    "\n",
    "    img_size:  int = 384\n",
    "    epochs:    int = 30\n",
    "    batch_size:int = 16\n",
    "    num_workers:int = 4\n",
    "\n",
    "    # Optimizasyon\n",
    "    lr: float = 3e-4\n",
    "    weight_decay: float = 0.05\n",
    "    min_lr: float = 1e-6\n",
    "    warmup_ratio: float = 0.1\n",
    "    grad_clip: float = 1.0\n",
    "    use_ema: bool = True\n",
    "    ema_decay: float = 0.999\n",
    "\n",
    "    # Loss/imbalance\n",
    "    loss_type: str = \"lsce\"     # \"lsce\" | \"focal\" | \"bce\"\n",
    "    label_smoothing: float = 0.1\n",
    "    focal_gamma: float = 2.0\n",
    "    use_weighted_sampler: bool = True\n",
    "    force_binary_bce: bool = False  # True ise 2 sınıfta bile BCE (num_classes=1)\n",
    "\n",
    "    # Early stop\n",
    "    patience: int = 7\n",
    "\n",
    "    # Seed\n",
    "    seed: int = 42\n",
    "\n",
    "CFG = CFG()\n",
    "os.makedirs(os.path.dirname(CFG.save_path) or \".\", exist_ok=True)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# =========================\n",
    "# Yardımcılar\n",
    "# =========================\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD  = [0.229, 0.224, 0.225]\n",
    "\n",
    "def set_seed(s: int):\n",
    "    random.seed(s); np.random.seed(s)\n",
    "    torch.manual_seed(s); torch.cuda.manual_seed_all(s)\n",
    "\n",
    "class EMA:\n",
    "    def __init__(self, model: nn.Module, decay: float = 0.999):\n",
    "        self.decay = decay\n",
    "        self.ema = deepcopy(model).eval()\n",
    "        for p in self.ema.parameters(): p.requires_grad_(False)\n",
    "    def to(self, dev): self.ema.to(dev); return self\n",
    "    @torch.no_grad()\n",
    "    def update(self, model: nn.Module):\n",
    "        d = self.decay\n",
    "        for ep, p in zip(self.ema.parameters(), model.parameters()):\n",
    "            ep.mul_(d).add_(p, alpha=1.0 - d)\n",
    "        for eb, b in zip(self.ema.buffers(), model.buffers()):\n",
    "            eb.copy_(b)\n",
    "\n",
    "class WarmupCosine:\n",
    "    def __init__(self, optimizer, warmup_steps, max_steps, min_lr=1e-6):\n",
    "        self.opt = optimizer\n",
    "        self.ws  = max(1, warmup_steps)\n",
    "        self.ms  = max_steps\n",
    "        self.min_lr = min_lr\n",
    "        self.step_n = 0\n",
    "        self.base = [g[\"lr\"] for g in optimizer.param_groups]\n",
    "    def step(self):\n",
    "        self.step_n += 1\n",
    "        for i, g in enumerate(self.opt.param_groups):\n",
    "            base_lr = self.base[i]\n",
    "            if self.step_n <= self.ws:\n",
    "                lr = base_lr * self.step_n / self.ws\n",
    "            else:\n",
    "                prog = (self.step_n - self.ws) / max(1, self.ms - self.ws)\n",
    "                lr = self.min_lr + 0.5*(base_lr - self.min_lr)*(1 + math.cos(math.pi * prog))\n",
    "            g[\"lr\"] = lr\n",
    "\n",
    "def build_transforms(img_size: int, train: bool):\n",
    "    ops = [transforms.Grayscale(num_output_channels=3)]\n",
    "    if train:\n",
    "        ops += [\n",
    "            transforms.RandomResizedCrop(img_size, scale=(0.8, 1.0), ratio=(0.9, 1.1)),\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.RandomRotation(10),\n",
    "            transforms.ColorJitter(brightness=0.1, contrast=0.1),\n",
    "        ]\n",
    "    else:\n",
    "        ops += [transforms.Resize((img_size, img_size))]\n",
    "    ops += [transforms.ToTensor(), transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD)]\n",
    "    return transforms.Compose(ops)\n",
    "\n",
    "def make_loaders() -> Tuple[DataLoader, DataLoader, int, np.ndarray]:\n",
    "    train_tf = build_transforms(CFG.img_size, True)\n",
    "    val_tf   = build_transforms(CFG.img_size, False)\n",
    "\n",
    "    train_ds = datasets.ImageFolder(CFG.train_dir, transform=train_tf)\n",
    "    val_ds   = datasets.ImageFolder(CFG.val_dir, transform=val_tf)\n",
    "\n",
    "    num_classes = len(train_ds.classes)\n",
    "    targets = np.array(train_ds.targets)\n",
    "\n",
    "    if CFG.use_weighted_sampler:\n",
    "        counts = np.bincount(targets, minlength=num_classes)\n",
    "        class_w = 1.0 / (counts + 1e-6)\n",
    "        sample_w = class_w[targets]\n",
    "        sampler = WeightedRandomSampler(sample_w.tolist(), num_samples=len(sample_w), replacement=True)\n",
    "        train_ld = DataLoader(train_ds, batch_size=CFG.batch_size, sampler=sampler,\n",
    "                              num_workers=CFG.num_workers, pin_memory=True, drop_last=True)\n",
    "    else:\n",
    "        train_ld = DataLoader(train_ds, batch_size=CFG.batch_size, shuffle=True,\n",
    "                              num_workers=CFG.num_workers, pin_memory=True, drop_last=True)\n",
    "\n",
    "    val_ld = DataLoader(val_ds, batch_size=CFG.batch_size, shuffle=False,\n",
    "                        num_workers=CFG.num_workers, pin_memory=True)\n",
    "\n",
    "    return train_ld, val_ld, num_classes, targets\n",
    "\n",
    "# =========================\n",
    "# Loss fonksiyonları\n",
    "# =========================\n",
    "class LabelSmoothingCE(nn.Module):\n",
    "    def __init__(self, eps: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "    def forward(self, logits, targets):\n",
    "        C = logits.size(-1)\n",
    "        logp = F.log_softmax(logits, dim=-1)\n",
    "        with torch.no_grad():\n",
    "            true = torch.zeros_like(logp).fill_(self.eps / C)\n",
    "            true.scatter_(1, targets.unsqueeze(1), 1.0 - self.eps + (self.eps / C))\n",
    "        return torch.mean(torch.sum(-true * logp, dim=-1))\n",
    "\n",
    "class FocalCrossEntropy(nn.Module):\n",
    "    def __init__(self, alpha=None, gamma: float = 2.0, reduction=\"mean\"):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "    def forward(self, logits, targets):\n",
    "        logp = F.log_softmax(logits, dim=1)\n",
    "        p = torch.exp(logp)\n",
    "        ce = F.nll_loss(logp, targets, reduction=\"none\")\n",
    "        pt = p[torch.arange(logits.size(0), device=logits.device), targets]\n",
    "        loss = (1 - pt) ** self.gamma * ce\n",
    "        if self.alpha is not None:\n",
    "            if isinstance(self.alpha, (list, tuple)):\n",
    "                a = torch.tensor(self.alpha, device=logits.device, dtype=logits.dtype)[targets]\n",
    "                loss = a * loss\n",
    "            else:\n",
    "                loss = self.alpha * loss\n",
    "        return loss.mean() if self.reduction == \"mean\" else loss.sum()\n",
    "\n",
    "class BinaryFocal(nn.Module):\n",
    "    def __init__(self, gamma=2.0, alpha=None):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "    def forward(self, logits, targets):\n",
    "        # logits (B,1), targets (B,)\n",
    "        probs = torch.sigmoid(logits).squeeze(1)\n",
    "        targets = targets.float()\n",
    "        ce = F.binary_cross_entropy(probs, targets, reduction=\"none\")\n",
    "        pt = torch.where(targets == 1, probs, 1 - probs)\n",
    "        loss = (1 - pt) ** self.gamma * ce\n",
    "        if self.alpha is not None:\n",
    "            loss = torch.where(targets == 1, self.alpha * loss, (1 - self.alpha) * loss)\n",
    "        return loss.mean()\n",
    "\n",
    "# =========================\n",
    "# Değerlendirme\n",
    "# =========================\n",
    "def evaluate(model: nn.Module, loader: DataLoader, num_classes: int, optimize_thr: bool):\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    y_prob_bin = []  # binary için olasılık\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in loader:\n",
    "            imgs = imgs.to(device, non_blocking=True)\n",
    "            labels = labels.to(device, non_blocking=True)\n",
    "            logits = model(imgs)\n",
    "            if num_classes == 1:\n",
    "                probs = torch.sigmoid(logits).squeeze(1)\n",
    "                y_prob_bin.append(probs.cpu().numpy())\n",
    "                # threshold daha sonra optimize edilecek\n",
    "            else:\n",
    "                preds = torch.argmax(logits, dim=1)\n",
    "                y_pred.append(preds.cpu().numpy())\n",
    "            y_true.append(labels.cpu().numpy())\n",
    "\n",
    "    y_true = np.concatenate(y_true)\n",
    "    if num_classes == 1:\n",
    "        y_prob = np.concatenate(y_prob_bin)\n",
    "        # eşik optimizasyonu (maks F1)\n",
    "        if optimize_thr:\n",
    "            ths = np.linspace(0, 1, 1001)\n",
    "            best_f1, best_th = -1, 0.5\n",
    "            for t in ths:\n",
    "                preds = (y_prob >= t).astype(int)\n",
    "                f1 = f1_score(y_true, preds)\n",
    "                if f1 > best_f1:\n",
    "                    best_f1, best_th = f1, t\n",
    "            y_pred = (y_prob >= best_th).astype(int)\n",
    "            acc = accuracy_score(y_true, y_pred)\n",
    "            rep = classification_report(y_true, y_pred, digits=4, target_names=[\"neg\",\"pos\"])\n",
    "            cm  = confusion_matrix(y_true, y_pred)\n",
    "            return acc, best_f1, y_true, y_pred, dict(threshold=best_th, report=rep, cm=cm.tolist())\n",
    "        else:\n",
    "            preds = (y_prob >= 0.5).astype(int)\n",
    "            f1 = f1_score(y_true, preds)\n",
    "            acc = accuracy_score(y_true, preds)\n",
    "            rep = classification_report(y_true, preds, digits=4, target_names=[\"neg\",\"pos\"])\n",
    "            cm  = confusion_matrix(y_true, preds)\n",
    "            return acc, f1, y_true, preds, dict(threshold=0.5, report=rep, cm=cm.tolist())\n",
    "    else:\n",
    "        y_pred = np.concatenate(y_pred)\n",
    "        f1 = f1_score(y_true, y_pred, average=\"macro\")\n",
    "        acc = accuracy_score(y_true, y_pred)\n",
    "        rep = classification_report(y_true, y_pred, digits=4)\n",
    "        cm  = confusion_matrix(y_true, y_pred)\n",
    "        return acc, f1, y_true, y_pred, dict(report=rep, cm=cm.tolist())\n",
    "\n",
    "# =========================\n",
    "# Eğitim\n",
    "# =========================\n",
    "def train():\n",
    "    set_seed(CFG.seed)\n",
    "\n",
    "    # Data\n",
    "    train_ld, val_ld, ds_num_classes, train_targets = make_loaders()\n",
    "\n",
    "    # Num classes belirle (binary BCE opsiyonu)\n",
    "    if ds_num_classes == 2 and CFG.force_binary_bce:\n",
    "        num_classes = 1\n",
    "    else:\n",
    "        num_classes = ds_num_classes\n",
    "\n",
    "    # Model\n",
    "    model = StrokeViT_R50(num_classes=num_classes, img_size=CFG.img_size).to(device)\n",
    "\n",
    "    # EMA\n",
    "    ema = EMA(model, decay=CFG.ema_decay).to(device) if CFG.use_ema else None\n",
    "\n",
    "    # Optimizer + Scheduler\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=CFG.lr, weight_decay=CFG.weight_decay)\n",
    "    total_steps = CFG.epochs * len(train_ld)\n",
    "    warmup_steps = int(CFG.warmup_ratio * total_steps)\n",
    "    scheduler = WarmupCosine(optimizer, warmup_steps=warmup_steps, max_steps=total_steps, min_lr=CFG.min_lr)\n",
    "\n",
    "    # Loss seçimi\n",
    "    if num_classes == 1 or CFG.loss_type == \"bce\":\n",
    "        # Binary BCE / Focal-BCE\n",
    "        if CFG.loss_type == \"bce\":\n",
    "            # pos_weight = neg/pos\n",
    "            neg = (train_targets == 0).sum()\n",
    "            pos = (train_targets == 1).sum()\n",
    "            pos_weight = torch.tensor([neg / max(1, pos)], device=device, dtype=torch.float32)\n",
    "            criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "        else:\n",
    "            criterion = BinaryFocal(gamma=CFG.focal_gamma, alpha=0.5)\n",
    "    else:\n",
    "        # Multi-class CE ailesi\n",
    "        if CFG.loss_type == \"focal\":\n",
    "            criterion = FocalCrossEntropy(gamma=CFG.focal_gamma)\n",
    "        else:\n",
    "            criterion = LabelSmoothingCE(eps=CFG.label_smoothing)\n",
    "\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    best_f1 = -1.0\n",
    "    best_payload = None\n",
    "    endure = 0\n",
    "\n",
    "    for epoch in range(1, CFG.epochs + 1):\n",
    "        model.train()\n",
    "        run_loss = 0.0\n",
    "\n",
    "        for imgs, labels in train_ld:\n",
    "            imgs = imgs.to(device, non_blocking=True)\n",
    "            labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            with autocast():\n",
    "                logits = model(imgs)\n",
    "                if num_classes == 1 or CFG.loss_type == \"bce\":\n",
    "                    loss = criterion(logits.squeeze(1), labels)\n",
    "                else:\n",
    "                    loss = criterion(logits, labels)\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=CFG.grad_clip)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            scheduler.step()\n",
    "\n",
    "            if ema: ema.update(model)\n",
    "            run_loss += loss.item()\n",
    "\n",
    "        # ---- Validation ----\n",
    "        eval_model = ema.ema if ema else model\n",
    "        acc, f1, y_true, y_pred, extra = evaluate(\n",
    "            eval_model, val_ld, num_classes=num_classes, optimize_thr=(num_classes==1)\n",
    "        )\n",
    "\n",
    "        print(f\"Epoch {epoch:02d} | TrainLoss {run_loss/len(train_ld):.4f} | \"\n",
    "              f\"ValAcc {acc:.4f} | ValF1 {f1:.4f}\")\n",
    "        if \"report\" in extra:\n",
    "            print(extra[\"report\"])\n",
    "\n",
    "        # En iyiyi kaydet\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            payload = {\n",
    "                \"epoch\": epoch,\n",
    "                \"model\": eval_model.state_dict(),\n",
    "                \"num_classes\": num_classes,\n",
    "                \"cfg\": CFG.__dict__,\n",
    "                \"val_f1\": float(f1),\n",
    "            }\n",
    "            if num_classes == 1 and \"threshold\" in extra:\n",
    "                payload[\"best_threshold\"] = float(extra[\"threshold\"])\n",
    "            torch.save(payload, CFG.save_path)\n",
    "            best_payload = payload\n",
    "            print(f\"✅ Best saved -> {CFG.save_path} (F1={best_f1:.4f})\")\n",
    "            endure = 0\n",
    "        else:\n",
    "            endure += 1\n",
    "            if endure >= CFG.patience:\n",
    "                print(\"⏹ Early stopping tetiklendi.\")\n",
    "                break\n",
    "\n",
    "    print(\"Training done.\")\n",
    "    if best_payload is not None:\n",
    "        meta_path = os.path.splitext(CFG.save_path)[0] + \"_summary.json\"\n",
    "        with open(meta_path, \"w\") as f:\n",
    "            json.dump({\n",
    "                \"epoch\": best_payload[\"epoch\"],\n",
    "                \"val_f1\": best_payload[\"val_f1\"],\n",
    "                \"num_classes\": best_payload[\"num_classes\"],\n",
    "                \"best_threshold\": best_payload.get(\"best_threshold\", 0.5),\n",
    "                \"cfg\": best_payload[\"cfg\"]\n",
    "            }, f, indent=2)\n",
    "        print(f\"✓ Özet yazıldı: {meta_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "647cb4fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/comp5/ARTEK/SYZ_25/ikinci_gorev_env/lib/python3.10/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ CSV yazıldı: test_r50_outputs_2gfgfgfrtyhfdryhfg/test_predictions_multiclass.csv\n",
      "✓ JSON yazıldı: test_r50_outputs_2gfgfgfrtyhfdryhfg/test_summary.json\n"
     ]
    }
   ],
   "source": [
    "# test_strokevit_r50_noarg.py\n",
    "# -*- coding: utf-8 -*-\n",
    "import os, json\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "from typing import Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torch.cuda.amp import autocast\n",
    "from PIL import Image\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, confusion_matrix, classification_report\n",
    ")\n",
    "\n",
    "from model_strokevit_r50 import StrokeViT_R50\n",
    "\n",
    "# ============== Konfig ==============\n",
    "@dataclass\n",
    "class CFG:\n",
    "    test_dir:  str = \"/home/comp5/ARTEK/SYZ_25/SYZ_25_Egitim/genel_veri/2025_Veri/test_veri_seti/ÜNİVERSİTEVEÜZERİ_EGİTİM 2_PNG/BT_TestSet/output_crop_gamma/test\"\n",
    "    ckpt_path: str = \"/home/comp5/ARTEK/SYZ_25/SYZ_25_Egitim/ILK_GOREV/nihai_modeller/resnet50_strokevit+/outputs_strokevit_r50_best/strokevit_r50_best.pth\"\n",
    "    out_dir:   str = \"test_r50_outputs_2gfgfgfrtyhfdryhfg\"\n",
    "\n",
    "    img_size:   int = 384\n",
    "    batch_size: int = 32\n",
    "    num_workers:int = 4\n",
    "\n",
    "    use_saved_threshold: bool = True    # binary ise summary/ckpt içindeki eşiği kullan\n",
    "    tta_hflip: bool = True              # TTA: yatay ayna ortalaması\n",
    "    save_csv: bool = True\n",
    "\n",
    "CFG = CFG()\n",
    "os.makedirs(CFG.out_dir, exist_ok=True)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD  = [0.229, 0.224, 0.225]\n",
    "\n",
    "# ============== Data ==============\n",
    "class ToRGB:\n",
    "    def __call__(self, img: Image.Image):\n",
    "        return img.convert(\"RGB\") if img.mode != \"RGB\" else img\n",
    "\n",
    "def make_test_loader() -> Tuple[datasets.ImageFolder, DataLoader]:\n",
    "    tfm = transforms.Compose([\n",
    "        ToRGB(),\n",
    "        transforms.Resize((CFG.img_size, CFG.img_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n",
    "    ])\n",
    "    ds = datasets.ImageFolder(CFG.test_dir, transform=tfm)\n",
    "    ld = DataLoader(ds, batch_size=CFG.batch_size, shuffle=False,\n",
    "                    num_workers=CFG.num_workers, pin_memory=True)\n",
    "    return ds, ld\n",
    "\n",
    "# ============== Yardımcılar ==============\n",
    "def load_model_and_meta():\n",
    "    # ckpt: {\"epoch\", \"model\", \"num_classes\", \"cfg\", \"val_f1\", (opt) \"best_threshold\"}\n",
    "    payload = torch.load(CFG.ckpt_path, map_location=\"cpu\")\n",
    "    num_classes = payload.get(\"num_classes\", None)\n",
    "    if num_classes is None:\n",
    "        # Eski kayıtlarda yoksa dataset'ten alacağız\n",
    "        print(\"[warn] num_classes checkpoint'te bulunamadı; dataset'e göre belirlenecek.\")\n",
    "    model = StrokeViT_R50(num_classes=num_classes if num_classes else 3, img_size=CFG.img_size)\n",
    "    state = payload[\"model\"] if \"model\" in payload else payload\n",
    "    model.load_state_dict(state, strict=True)\n",
    "    model.to(device).eval()\n",
    "\n",
    "    # threshold okuma (binary ise)\n",
    "    best_th = payload.get(\"best_threshold\", None)\n",
    "    # summary.json varsa tercih edebilirsin\n",
    "    sum_path = os.path.splitext(CFG.ckpt_path)[0] + \"_summary.json\"\n",
    "    if CFG.use_saved_threshold and os.path.exists(sum_path):\n",
    "        try:\n",
    "            with open(sum_path, \"r\") as f:\n",
    "                j = json.load(f)\n",
    "            best_th = float(j.get(\"best_threshold\", best_th if best_th is not None else 0.5))\n",
    "        except Exception:\n",
    "            pass\n",
    "    return model, num_classes, best_th\n",
    "\n",
    "def horizontal_flip_batch(x: torch.Tensor) -> torch.Tensor:\n",
    "    return torch.flip(x, dims=[3])\n",
    "\n",
    "# ============== Test / İnferans ==============\n",
    "def run_test():\n",
    "    ds, ld = make_test_loader()\n",
    "    model, num_classes_ckpt, best_th = load_model_and_meta()\n",
    "\n",
    "    # Eğer ckpt'te num_classes yoksa dataset'ten belirle\n",
    "    num_classes = num_classes_ckpt if num_classes_ckpt is not None else len(ds.classes)\n",
    "    if num_classes == 2:\n",
    "        # iki sınıflı ama BCE ile eğitilmiş olabilir; ckpt'te num_classes=1 ise zaten yukarıda gelirdi\n",
    "        # burada 2 sınıfı çok-sınıf olarak değerlendiriyoruz (softmax). Binary eşiği sadece ckpt num_classes==1 iken gerekli.\n",
    "        pass\n",
    "\n",
    "    all_true, all_pred = [], []\n",
    "    all_prob_bin = []     # binary için pozitif olasılık\n",
    "    all_top1prob = []     # çok-sınıf için top-1 olasılık\n",
    "    all_paths = []\n",
    "\n",
    "    with torch.no_grad(), autocast():\n",
    "        for imgs, labels in ld:\n",
    "            imgs = imgs.to(device, non_blocking=True)\n",
    "            logits = None\n",
    "            if CFG.tta_hflip:\n",
    "                imgs_flip = horizontal_flip_batch(imgs)\n",
    "                logits = model(imgs)\n",
    "                logits_flip = model(imgs_flip)\n",
    "                logits = 0.5 * (logits + logits_flip)\n",
    "            else:\n",
    "                logits = model(imgs)\n",
    "\n",
    "            if num_classes == 1:\n",
    "                probs = torch.sigmoid(logits).squeeze(1)  # (B,)\n",
    "                all_prob_bin.append(probs.cpu().numpy())\n",
    "            else:\n",
    "                probs = torch.softmax(logits, dim=1)       # (B,C)\n",
    "                top1 = probs.max(dim=1).values\n",
    "                preds = probs.argmax(dim=1)\n",
    "                all_pred.append(preds.cpu().numpy())\n",
    "                all_top1prob.append(top1.cpu().numpy())\n",
    "\n",
    "            all_true.append(labels.numpy())\n",
    "            # ImageFolder sample paths\n",
    "            # Mini-trick: DataLoader batch sırası DataSet.samples sırasını takip eder\n",
    "            # Yine de güvenli olmak için her iterasyonda batch sayısı kadar path topla\n",
    "            start = len(all_paths)\n",
    "            for _ in range(labels.size(0)):\n",
    "                # sıralı ilerlediğimizi varsayalım\n",
    "                pass\n",
    "        # Daha güvenlisi: doğrudan dataset.samples'tan topla:\n",
    "    all_paths = [p for (p, _) in ds.samples]\n",
    "\n",
    "    y_true = np.concatenate(all_true)\n",
    "\n",
    "    out_json = {\n",
    "        \"classes\": ds.classes,\n",
    "        \"num_classes\": int(num_classes),\n",
    "        \"used_threshold\": None,\n",
    "        \"metrics\": {},\n",
    "        \"confusion_matrix\": None,\n",
    "    }\n",
    "\n",
    "    # --- Tahmin ve metrikler ---\n",
    "    if num_classes == 1:\n",
    "        y_prob = np.concatenate(all_prob_bin)\n",
    "        # Eşik seçimi:\n",
    "        if CFG.use_saved_threshold and best_th is not None:\n",
    "            th = float(best_th)\n",
    "        else:\n",
    "            # test set üzerinde maksimum macro-F1 eşiği (isteğe bağlı)\n",
    "            ths = np.linspace(0, 1, 1001)\n",
    "            best_f1, th = -1, 0.5\n",
    "            for t in ths:\n",
    "                y_pred_tmp = (y_prob >= t).astype(int)\n",
    "                macro_f1_tmp = f1_score(y_true, y_pred_tmp, average=\"macro\")\n",
    "                if macro_f1_tmp > best_f1:\n",
    "                    best_f1, th = macro_f1_tmp, t\n",
    "        y_pred = (y_prob >= th).astype(int)\n",
    "\n",
    "        acc = accuracy_score(y_true, y_pred)\n",
    "        macro_f1 = f1_score(y_true, y_pred, average=\"macro\")\n",
    "        bin_f1 = f1_score(y_true, y_pred)  # pozitif sınıf için F1\n",
    "        cm = confusion_matrix(y_true, y_pred).tolist()\n",
    "        rep = classification_report(y_true, y_pred, digits=4, target_names=(ds.classes if len(ds.classes)==2 else [\"neg\",\"pos\"]))\n",
    "\n",
    "        out_json[\"used_threshold\"] = float(th)\n",
    "        out_json[\"metrics\"] = {\n",
    "            \"accuracy\": float(acc),\n",
    "            \"macro_f1\": float(macro_f1),\n",
    "            \"binary_f1\": float(bin_f1),\n",
    "            \"report\": rep,\n",
    "        }\n",
    "        out_json[\"confusion_matrix\"] = cm\n",
    "\n",
    "        # CSV\n",
    "        if CFG.save_csv:\n",
    "            import csv\n",
    "            csv_path = os.path.join(CFG.out_dir, \"test_predictions_binary.csv\")\n",
    "            with open(csv_path, \"w\", newline=\"\") as f:\n",
    "                w = csv.writer(f)\n",
    "                w.writerow([\"path\",\"true_idx\",\"true_name\",\"prob_pos\",\"pred_idx\",\"pred_name\",\"threshold\"])\n",
    "                for (path, t), pr, pd in zip(ds.samples, y_prob, y_pred):\n",
    "                    tname = ds.classes[t] if t < len(ds.classes) else str(t)\n",
    "                    pname = ds.classes[pd] if pd < len(ds.classes) else str(pd)\n",
    "                    w.writerow([path, int(t), tname, f\"{pr:.6f}\", int(pd), pname, f\"{th:.3f}\"])\n",
    "            print(f\"✓ CSV yazıldı: {csv_path}\")\n",
    "\n",
    "    else:\n",
    "        y_pred = np.concatenate(all_pred)\n",
    "        acc = accuracy_score(y_true, y_pred)\n",
    "        macro_f1 = f1_score(y_true, y_pred, average=\"macro\")\n",
    "        cm = confusion_matrix(y_true, y_pred).tolist()\n",
    "        rep = classification_report(y_true, y_pred, digits=4, target_names=ds.classes)\n",
    "\n",
    "        out_json[\"metrics\"] = {\n",
    "            \"accuracy\": float(acc),\n",
    "            \"macro_f1\": float(macro_f1),\n",
    "            \"report\": rep,\n",
    "        }\n",
    "        out_json[\"confusion_matrix\"] = cm\n",
    "\n",
    "        # CSV\n",
    "        if CFG.save_csv:\n",
    "            import csv\n",
    "            top1 = np.concatenate(all_top1prob)\n",
    "            csv_path = os.path.join(CFG.out_dir, \"test_predictions_multiclass.csv\")\n",
    "            with open(csv_path, \"w\", newline=\"\") as f:\n",
    "                w = csv.writer(f)\n",
    "                w.writerow([\"path\",\"true_idx\",\"true_name\",\"pred_idx\",\"pred_name\",\"top1_prob\"])\n",
    "                for (path, t), pd, p1 in zip(ds.samples, y_pred, top1):\n",
    "                    w.writerow([path, int(t), ds.classes[t], int(pd), ds.classes[pd], f\"{p1:.6f}\"])\n",
    "            print(f\"✓ CSV yazıldı: {csv_path}\")\n",
    "\n",
    "    # JSON sonuç\n",
    "    json_path = os.path.join(CFG.out_dir, \"test_summary.json\")\n",
    "    with open(json_path, \"w\") as f:\n",
    "        json.dump(out_json, f, indent=2)\n",
    "    print(f\"✓ JSON yazıldı: {json_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_test()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ikinci_gorev_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
